{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12450, 23)\n",
      "0.5330923694779116\n",
      "0.5658905430009639\n",
      "0.0027420343675529394\n",
      "0.5663090266623836\n",
      "0.0027420343675529394\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Import lots of tools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# load default data set\n",
    "credit = pd.read_csv(\"defaultBal.csv\")\n",
    "# set up data for scikit-learn\n",
    "# note:  You don't really have to use X,y\n",
    "# this is our target\n",
    "y = credit.default\n",
    "# These are predictors (skip id)\n",
    "Xall= credit.values[:,1:24]\n",
    "# copy of all data to predictor\n",
    "X = Xall.copy()\n",
    "# Set up as just real values, use this for last part of problem\n",
    "# Restricts predictor to only real values (not discrete)\n",
    "# X = Xall[:,12:23].copy()\n",
    "print(X.shape)\n",
    "print(np.mean(y))\n",
    "yGuess = np.mean(y)\n",
    "\n",
    "# start monte-carlo for GaussianNB()\n",
    "nmc = 50\n",
    "trainScore = np.zeros(nmc)\n",
    "testScore  = np.zeros(nmc)\n",
    "gnb = GaussianNB()\n",
    "for i in range(nmc):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25)\n",
    "    trainFit = gnb.fit(X_train,y_train)\n",
    "    trainScore[i] = trainFit.score(X_train,y_train)\n",
    "    testScore[i] =  trainFit.score(X_test,y_test)\n",
    "print(np.mean(trainScore))\n",
    "print(np.std(trainScore))\n",
    "print(np.mean(testScore))\n",
    "print(np.std(trainScore))\n",
    "print(np.mean(testScore>yGuess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Discriminant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6520681160972476\n",
      "0.003651540490871438\n",
      "0.6480436877610022\n",
      "0.003651540490871438\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "nmc = 50\n",
    "trainScore = np.zeros(nmc)\n",
    "testScore  = np.zeros(nmc)\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "for i in range(nmc):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25)\n",
    "    trainFit = lda.fit(X_train,y_train)\n",
    "    trainScore[i] = trainFit.score(X_train,y_train)\n",
    "    testScore[i] =  trainFit.score(X_test,y_test)\n",
    "print(np.mean(trainScore))\n",
    "print(np.std(trainScore))\n",
    "print(np.mean(testScore))\n",
    "print(np.std(trainScore))\n",
    "print(np.mean(testScore>yGuess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for C = 100\n",
      "0.6302067045089429\n",
      "0.0072645461620794586\n",
      "0.6274076453581754\n",
      "0.0074724306578111965\n",
      "1.0\n",
      "for C = 1\n",
      "0.6307015101210239\n",
      "0.006285055326383176\n",
      "0.6293029232251847\n",
      "0.00859982196227048\n",
      "1.0\n",
      "for C = 0.01\n",
      "0.6311384813109137\n",
      "0.0068240399239194996\n",
      "0.6299839383231609\n",
      "0.00899522965546473\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "nmc = 50\n",
    "train_ =[]\n",
    "test_ =[]\n",
    "c_range = [100,1,0.01]\n",
    "\n",
    "test_plt = []\n",
    "train_plt = []\n",
    "for m in c_range:\n",
    "  train_ = []\n",
    "  test_ = []\n",
    "  for i in range(nmc):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.25) \n",
    "    trainFit = LogisticRegression(C=m).fit(X_train,y_train)\n",
    "    train_.append(trainFit.score(X_train,y_train))\n",
    "    test_.append(trainFit.score(X_test,y_test))\n",
    "\n",
    "  print(\"for C = \"+ str(m))  \n",
    "  print(np.mean(train_))\n",
    "  print(np.std(train_))\n",
    "  print(np.mean(test_))\n",
    "  print(np.std(test_))\n",
    "  print(np.mean(testScore>yGuess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearSVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for C = 100\n",
      "0.5472421548677304\n",
      "0.03321557695990959\n",
      "0.5483199486026341\n",
      "0.03193525656200487\n",
      "1.0\n",
      "for C = 1\n",
      "0.5350819320981043\n",
      "0.033254724877683196\n",
      "0.5327786700931578\n",
      "0.03416802805478047\n",
      "1.0\n",
      "for C = 0.01\n",
      "0.5439605869122844\n",
      "0.03518590420888528\n",
      "0.5408352071956313\n",
      "0.034952800422611015\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "nmc = 50\n",
    "train_ =[]\n",
    "test_ =[]\n",
    "c_range = [100,1,0.01]\n",
    "\n",
    "test_plt = []\n",
    "train_plt = []\n",
    "for m in c_range:\n",
    "  train_ = []\n",
    "  test_ = []\n",
    "  for i in range(nmc):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.25) \n",
    "    trainFit = LinearSVC(C=m).fit(X_train,y_train)\n",
    "    train_.append(trainFit.score(X_train,y_train))\n",
    "    test_.append(trainFit.score(X_test,y_test))\n",
    "\n",
    "  print(\"for C = \"+ str(m))  \n",
    "  print(np.mean(train_))\n",
    "  print(np.std(train_))\n",
    "  print(np.mean(test_))\n",
    "  print(np.std(test_))\n",
    "  print(np.mean(testScore>yGuess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNeighborClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for n_neighbors = 3\n",
      "0.7786012637892257\n",
      "0.0025924584376835766\n",
      "0.5720591069707678\n",
      "0.007335300111785214\n",
      "1.0\n",
      "for n_neighbors = 11\n",
      "0.68014994109457\n",
      "0.003198364331854822\n",
      "0.6001413427561837\n",
      "0.008184727936833253\n",
      "1.0\n",
      "for n_neighbors = 25\n",
      "0.6497590232408696\n",
      "0.002801704048279742\n",
      "0.6092515258592998\n",
      "0.0072025115103072305\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "nmc = 50\n",
    "train_ =[]\n",
    "test_ =[]\n",
    "n_neighbors = [3, 11, 25]\n",
    "\n",
    "test_plt = []\n",
    "train_plt = []\n",
    "for m in n_neighbors:\n",
    "  train_ = []\n",
    "  test_ = []\n",
    "  for i in range(nmc):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.25) \n",
    "    trainFit = KNeighborsClassifier(n_neighbors=m).fit(X_train,y_train)\n",
    "    train_.append(trainFit.score(X_train,y_train))\n",
    "    test_.append(trainFit.score(X_test,y_test))\n",
    "\n",
    "  print(\"for n_neighbors = \"+ str(m))  \n",
    "  print(np.mean(train_))\n",
    "  print(np.std(train_))\n",
    "  print(np.mean(test_))\n",
    "  print(np.std(test_))\n",
    "  print(np.mean(testScore>yGuess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for max_depth = 5\n",
      "0.7043911320552639\n",
      "0.003040247692369075\n",
      "0.686765178284613\n",
      "0.008336551840580332\n",
      "1.0\n",
      "for max_depth = 10\n",
      "0.7779179608011139\n",
      "0.0053645878238368985\n",
      "0.6755348538387408\n",
      "0.00855239513870774\n",
      "1.0\n",
      "for max_depth = 25\n",
      "0.9894976973331906\n",
      "0.004657562174066489\n",
      "0.6259685191133955\n",
      "0.007647722520545187\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "nmc = 50\n",
    "train_ =[]\n",
    "test_ =[]\n",
    "max_depth = [5, 10, 25]\n",
    "test_plt = []\n",
    "train_plt = []\n",
    "for m in max_depth:\n",
    "  train_ = []\n",
    "  test_ = []\n",
    "  for i in range(nmc):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.25) \n",
    "    trainFit = DecisionTreeClassifier(max_depth=m).fit(X_train,y_train)\n",
    "    train_.append(trainFit.score(X_train,y_train))\n",
    "    test_.append(trainFit.score(X_test,y_test))\n",
    "\n",
    "  print(\"for max_depth = \"+ str(m))  \n",
    "  print(np.mean(train_))\n",
    "  print(np.std(train_))\n",
    "  print(np.mean(test_))\n",
    "  print(np.std(test_))\n",
    "  print(np.mean(testScore>yGuess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearDiscriminant for real valued data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5753218378494163\n",
      "0.004538813872742111\n",
      "0.5757211692900739\n",
      "0.004538813872742111\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "X = Xall[:,12:23].copy()\n",
    "nmc = 50\n",
    "trainScore = np.zeros(nmc)\n",
    "testScore  = np.zeros(nmc)\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "for i in range(nmc):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25)\n",
    "    trainFit = lda.fit(X_train,y_train)\n",
    "    trainScore[i] = trainFit.score(X_train,y_train)\n",
    "    testScore[i] =  trainFit.score(X_test,y_test)\n",
    "print(np.mean(trainScore))\n",
    "print(np.std(trainScore))\n",
    "print(np.mean(testScore))\n",
    "print(np.std(trainScore))\n",
    "print(np.mean(testScore>yGuess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the mean test score of above(part 1-5 & GaussianNB), Decision tree with max_depth= 5 has a best performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
